{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8lDQuQEnS8_"
      },
      "source": [
        "\n",
        "\n",
        "## OpenAI Gym's Taxi solution based on https://www.gocoder.one/blog/rl-tutorial-with-openai-gym/. Test 2 different hyperparameter sets and also 1 where we do each state+action pair no more than 10 times"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aWkaLM_o2pH"
      },
      "source": [
        "# ðŸ‹ï¸ Installing OpenAI Gym andÂ Taxi\n",
        "\n",
        "We'll be using the 'Taxi-v3' environment for this tutorial. To install gym (and numpy for later), run the cell below:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyjebRSHnK1F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ae4d4f2-d501-4dd7-80cc-07a02cc9290e"
      },
      "source": [
        "!pip install gym\n",
        "!pip install numpy"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym) (0.0.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAJK0BtorfEO"
      },
      "source": [
        "Next, import gym (and additional libraries that will be useful for later):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7As7qh4navx"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# used to help with visualizing in Colab\n",
        "from IPython.display import display, clear_output\n",
        "from time import sleep"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyMhgh3RsLWk"
      },
      "source": [
        "Gym contains a large library of different environments. Let's create the Taxi-v3 environment:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpYHWA95y_QJ",
        "outputId": "ba727018-dfd5-4579-d4c2-9502093bb2a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# create Taxi environment\n",
        "env = gym.make('Taxi-v3')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWMjVABSsnWT"
      },
      "source": [
        "# ðŸŽ² RandomÂ Agent\n",
        "\n",
        "We'll start by implementing an agent that doesn't learn at all. Instead, it will select actions at random. This will be our baseline.\n",
        "\n",
        "The first step is to give our agent the initial observation of the state. A state tells our agent what the current environment looks like. In Taxi, a state defines the current positions of the taxi, passenger, and pick-up and drop-off locations. Below are examples of three different states for Taxi:\n",
        "\n",
        "![taxi states](https://www.gocoder.one/static/taxi-states-0aad1b011cf3fe07b571712f2123335c.png \"Different Taxi states\")\n",
        "\n",
        "Note: Yellow = taxi, Blue letter = pickup location, Purple letter = drop-off destination\n",
        "\n",
        "To get the initial state:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNlV-YvdnlOH",
        "outputId": "801492f1-9db0-4217-b2a0-f0c8e95e61c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# create a new instance of taxi, and get the initial state\n",
        "state = env.reset()\n",
        "\n",
        "print(f\"Initial state: {state}\")\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial state: 149\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WM7GRNHnvRaH"
      },
      "source": [
        "Next, we'll run a for-loop to cycle through the game. At each iteration, our agent will:\n",
        "\n",
        "1. Make a random action from the action space (0â€Š-â€Šsouth, 1â€Š-â€Šnorth, 2â€Š-â€Šeast, 3â€Š-â€Šwest, 4â€Š-â€Špick-up, 5â€Š-â€Šdrop-off)\n",
        "2. Receive the new state\n",
        "\n",
        "Here's our random agent:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aycxOXzQnoLU",
        "outputId": "a4a65c24-e7ad-48a7-928c-4c4609786f6c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "num_steps = 99\n",
        "for s in range(num_steps+1):\n",
        "\n",
        "    clear_output(wait=True)\n",
        "\n",
        "    print(f\"step: {s} out of {num_steps}\")\n",
        "\n",
        "    # sample a random action from the list of available actions\n",
        "    action = env.action_space.sample()\n",
        "\n",
        "    # perform this action on the environment\n",
        "    env.step(action)\n",
        "\n",
        "    # print the new state\n",
        "    env.render()\n",
        "\n",
        "    sleep(0.2)\n",
        "\n",
        "# end this instance of the taxi environment\n",
        "env.close()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step: 99 out of 99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8I-Whw1WxDra"
      },
      "source": [
        "When you run the cell above, you should see your agent making random moves. Not super exciting, but hopefully this helped you get familiar with the OpenAI Gym toolkit.\n",
        "\n",
        "Next, we'll implement the Q-learning algorithm that will enable our agent to learn from rewards."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVvgYgquUKBg",
        "outputId": "f505a2be-7359-40bd-b4ed-dbe96c6dd3fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "state_size = env.observation_space.n  # total number of states (S)\n",
        "action_size = env.action_space.n      # total number of actions (A)\n",
        "\n",
        "# initialize a qtable with 0's for all Q-values\n",
        "qtable = np.zeros((state_size, action_size))\n",
        "\n",
        "print(f\"Q table: {qtable}\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q table: [[0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Log_fB49uIHm"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0uvTrFXmJk7"
      },
      "source": [
        "As our agent explores, it will update the Q-table with the Q-values it finds. To calculate our Q-values, we'll introduce the Q-learning algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdpsBFdJm9ve"
      },
      "source": [
        "Here's our implementation of the Q-learning algorithm:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsOWYTX4VsDz",
        "outputId": "204dc261-4716-4077-e35d-9d58727582a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# hyperparameters to tune\n",
        "learning_rate = 0.9\n",
        "discount_rate = 0.8\n",
        "\n",
        "# dummy variables\n",
        "reward = 10 # R_(t+1)\n",
        "state = env.observation_space.sample()      # S_t\n",
        "action = env.action_space.sample()          # A_t\n",
        "new_state = env.observation_space.sample()  # S_(t+1)\n",
        "\n",
        "# Qlearning algorithm: Q(s,a) := Q(s,a) + learning_rate * (reward + discount_rate * max Q(s',a') - Q(s,a))\n",
        "qtable[state, action] += learning_rate * (reward + discount_rate * np.max(qtable[new_state,:]) - qtable[state,action])\n",
        "\n",
        "print(f\"Q-value for (state, action) pair ({state}, {action}): {qtable[state,action]}\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q-value for (state, action) pair (165, 2): 9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bx5IMA1idW9X"
      },
      "source": [
        "## Exploration vs Exploitation Trade-off\n",
        "\n",
        "We can let our agent explore to update our Q-table using the Q-learning algorithm. As our agent learns more about the environment, we can let it use this knowledge to take more optimal actions and converge fasterâ€Š-â€Šknown as **exploitation**.\n",
        "\n",
        "During exploitation, our agent will look at its Q-table and select the action with the highest Q-value (instead of a random action). Over time, our agent will need to explore less, and start exploiting what it knows instead.\n",
        "\n",
        "Here's our implementation of an exploration-exploitation strategy:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3aorBEvYdSLr"
      },
      "source": [
        "# dummy variables\n",
        "episode = random.randint(0,500)\n",
        "qtable = np.random.randn(env.observation_space.sample(), env.action_space.sample())\n",
        "\n",
        "# hyperparameters\n",
        "epsilon = 1.0     # probability that our agent will explore\n",
        "decay_rate = 0.01 # of epsilon\n",
        "\n",
        "if random.uniform(0,1) < epsilon:\n",
        "    # explore\n",
        "    action = env.action_space.sample()\n",
        "else:\n",
        "    # exploit\n",
        "    action = np.argmax(qtable[state,:])\n",
        "\n",
        "# epsilon decreases exponentially --> our agent will explore less and less\n",
        "epsilon = np.exp(-decay_rate*episode)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_zL2Vrd1yLv"
      },
      "source": [
        "In the example above, we set some value `epsilon` between 0 and 1. If `epsilon` is 0.7, there is a 70% chance that on this step our agent will explore instead of exploit. `epsilon` exponentially decays with each step, so that our agent explores less and less over time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5mA3SflarKs"
      },
      "source": [
        "# Bringing it allÂ together\n",
        "\n",
        "We're done with all the building blocks needed for our reinforcement learning agent. The process for training our agent will look like:\n",
        "\n",
        "1. Initialising our Q-table with 0's for all Q-values\n",
        "2. Let our agent play Taxi over a large number of games\n",
        "3. Continuously update the Q-table using the Q-learning algorithm and an exploration-exploitation strategy\n",
        "\n",
        "Here's the full implementation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SiXAK2OdpteR",
        "outputId": "090e7855-804a-437a-bca1-9b2155c1f8a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "class bcolors:\n",
        "    RED= '\\u001b[31m'\n",
        "    GREEN= '\\u001b[32m'\n",
        "    RESET= '\\u001b[0m'\n",
        "\n",
        "# create Taxi environment\n",
        "env = gym.make('Taxi-v3')\n",
        "\n",
        "# initialize q-table\n",
        "state_size = env.observation_space.n\n",
        "action_size = env.action_space.n\n",
        "qtable = np.zeros((state_size, action_size))\n",
        "\n",
        "# hyperparameters\n",
        "#learning_rate = 0.9\n",
        "#discount_rate = 0.8\n",
        "#epsilon = 1.0\n",
        "#decay_rate= 0.005\n",
        "\n",
        "#1\n",
        "# learning_rate = 0.6\n",
        "# discount_rate = 0.9\n",
        "# epsilon = 0.8\n",
        "# decay_rate= 0.000\n",
        "\n",
        "#2\n",
        "learning_rate = 0.9\n",
        "discount_rate = 0.8\n",
        "epsilon = 1.0\n",
        "decay_rate= 0.01\n",
        "\n",
        "# 3 - Initialize visitation table\n",
        "visitation_table = np.zeros((state_size, action_size))\n",
        "max_visits = 10 # maximum number of visits per state-action pair\n",
        "\n",
        "# training variables\n",
        "num_episodes = 2000\n",
        "\n",
        "# Visit each episode 99 times\n",
        "max_steps = 99\n",
        "\n",
        "print(\"AGENT IS TRAINING...\")\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "\n",
        "\t# Reset the environment\n",
        "\tstate = env.reset()\n",
        "\tstep = 0\n",
        "\tdone = False\n",
        "\n",
        "\tfor step in range(max_steps):\n",
        "\n",
        "\t\t# Exploration-exploitation tradeoff\n",
        "\t\tif random.uniform(0,1) < epsilon:\n",
        "\t\t\t# Explore\n",
        "      # action = env.action_space.sample()\n",
        "\n",
        "\t\t\t# 3 Get all actions that have been visited less than max_visits times for this state\n",
        "\t\t\tpossible_actions = [action for action in range(action_size) if visitation_table[state, action] < max_visits]\n",
        "\t\t\tif possible_actions:\n",
        "\t\t\t\t\t# If there are any such actions, randomly choose from them\n",
        "\t\t\t\t\taction = random.choice(possible_actions)\n",
        "\t\t\telse:\n",
        "\t\t\t\t\t# Otherwise, default to a random action (or could choose another strategy)\n",
        "\t\t\t\t\taction = env.action_space.sample()\n",
        "\t\telse:\n",
        "\t\t\t# Exploit\n",
        "\t\t\taction = np.argmax(qtable[state,:])\n",
        "\n",
        "\t\t# Take an action and observe the reward\n",
        "\t\tnew_state, reward, done, info = env.step(action)\n",
        "\n",
        "\t\t# Q-learning algorithm\n",
        "\t\tqtable[state,action] = qtable[state,action] + learning_rate * (reward + discount_rate * np.max(qtable[new_state,:])-qtable[state,action])\n",
        "\n",
        "\t\t# Update to our new state\n",
        "\t\tstate = new_state\n",
        "\n",
        "\t\t# if done, finish episode\n",
        "\t\tif done == True:\n",
        "\t\t\tbreak\n",
        "\n",
        "\t# Decrease epsilon\n",
        "\tepsilon = np.exp(-decay_rate*episode)\n",
        "\n",
        "# Get ready to watch our trained agent\n",
        "clear_output()\n",
        "print(f\"Our Q-table: {qtable}\")\n",
        "print(f\"Training completed over {num_episodes} episodes\")\n",
        "input(\"Press Enter to see our trained taxi agent\")\n",
        "sleep(1)\n",
        "clear_output()\n",
        "\n",
        "episodes_to_preview = 3\n",
        "for episode in range(episodes_to_preview):\n",
        "\n",
        "\t# Reset the environment\n",
        "\tstate = env.reset()\n",
        "\tstep = 0\n",
        "\tdone = False\n",
        "\tepisode_rewards = 0\n",
        "\n",
        "\tfor step in range(num_steps):\n",
        "\t\t# clear screen\n",
        "\t\tclear_output(wait=True)\n",
        "\n",
        "\t\tprint(f\"TRAINED AGENT\")\n",
        "\t\tprint(f\"+++++EPISODE {episode+1}+++++\")\n",
        "\t\tprint(f\"Step {step+1}\")\n",
        "\n",
        "\t\t# Exploit\n",
        "\t\taction = np.argmax(qtable[state,:])\n",
        "\n",
        "\t\t# Take an action and observe the reward\n",
        "\t\tnew_state, reward, done, info = env.step(action)\n",
        "\n",
        "\t\t# Accumulate our rewards\n",
        "\t\tepisode_rewards += reward\n",
        "\n",
        "\t\tenv.render()\n",
        "\t\tprint(\"\")\n",
        "\t\tif episode_rewards < 0:\n",
        "\t\t\tprint(f\"Score: {bcolors.RED}{episode_rewards}{bcolors.RESET}\")\n",
        "\t\telse:\n",
        "\t\t\tprint(f\"Score: {bcolors.GREEN}{episode_rewards}{bcolors.RESET}\")\n",
        "\t\tsleep(0.5)\n",
        "\n",
        "\t\t# Update to our new state\n",
        "\t\tstate = new_state\n",
        "\n",
        "\t\t# if done, finish episode\n",
        "\t\tif done == True:\n",
        "\t\t\tbreak\n",
        "\n",
        "# Close the Taxi environment\n",
        "env.close()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAINED AGENT\n",
            "+++++EPISODE 3+++++\n",
            "Step 13\n",
            "\n",
            "Score: \u001b[32m8\u001b[0m\n"
          ]
        }
      ]
    }
  ]
}